{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "HEIGHT = 32\n",
    "WIDTH  = 32\n",
    "DEPTH  = 3\n",
    "NUM_CLASSES = 10\n",
    "NUM_TRAIN_IMAGES = 40000\n",
    "NUM_VALID_IMAGES = 10000\n",
    "NUM_TEST_IMAGES  = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess_fn(image):\n",
    "\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, HEIGHT + 8, WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [HEIGHT, WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(filenames, batch_size):\n",
    "    \"\"\"Read the images and labels from 'filenames'.\"\"\"\n",
    "    # Repeat infinitely.\n",
    "    dataset = tf.data.TFRecordDataset(filenames).repeat()\n",
    "\n",
    "    # Parse records.\n",
    "    dataset = dataset.map(single_example_parser, num_parallel_calls=1)\n",
    "\n",
    "    # Batch it up.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    image_batch, label_batch = iterator.get_next()\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_example_parser(serialized_example):\n",
    "    \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "    # Dimensions of the images in the CIFAR-10 dataset.\n",
    "    # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n",
    "    # input format.\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image.set_shape([DEPTH * HEIGHT * WIDTH])\n",
    "\n",
    "    # Reshape from [depth * height * width] to [depth, height, width].\n",
    "    image = tf.cast(\n",
    "        tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),\n",
    "        tf.float32)\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    image = train_preprocess_fn(image)\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_model(input_shape):\n",
    "\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                      weights='imagenet',\n",
    "                                                      input_tensor=input_tensor,\n",
    "                                                      input_shape=input_shape,\n",
    "                                                      classes=None)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    predictions = Dense(10, activation='softmax')(x)\n",
    "    mdl = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def main(args):\n",
    "    # Hyper-parameters\n",
    "    epochs = args.epochs\n",
    "    lr = args.learning_rate\n",
    "    batch_size = args.batch_size\n",
    "    momentum = args.momentum\n",
    "    weight_decay = args.weight_decay\n",
    "    optimizer = args.optimizer\n",
    "\n",
    "    # Data directories and other options\n",
    "    gpu_count = args.gpu_count\n",
    "    training_dir = args.training\n",
    "    validation_dir = args.validation\n",
    "    eval_dir = args.eval\n",
    "\n",
    "    train_dataset = make_batch(training_dir,  batch_size)\n",
    "    val_dataset = make_batch(validation_dir, batch_size)\n",
    "    eval_dataset = make_batch(eval_dir, batch_size)\n",
    "\n",
    "    input_shape = (HEIGHT, WIDTH, DEPTH)\n",
    "    model = cifar10_model(input_shape)\n",
    "\n",
    "    # Multi-GPU training\n",
    "    if gpu_count > 1:\n",
    "        model = multi_gpu_model(model, gpus=gpu_count)\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(lr=lr, decay=weight_decay, momentum=momentum)\n",
    "    else:\n",
    "        opt = Adam(lr=lr, decay=weight_decay)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(x=train_dataset[0], y=train_dataset[1],\n",
    "                        steps_per_epoch=NUM_TRAIN_IMAGES // batch_size,\n",
    "                        validation_data=val_dataset,\n",
    "                        validation_steps=NUM_VALID_IMAGES // batch_size,\n",
    "                        epochs=epochs)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    score = model.evaluate(eval_dataset[0],\n",
    "                           eval_dataset[1],\n",
    "                           steps=NUM_TEST_IMAGES // args.batch_size,\n",
    "                           verbose=0)\n",
    "    print('Test loss    :', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # Save model to model directory\n",
    "    #tf.contrib.saved_model.save_keras_model(model, args.model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 01:56:00.487950 140411681085184 deprecation.py:323] From <ipython-input-3-281db9a6ad1c>:11: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n",
      "W1014 01:56:08.129960 140411681085184 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1014 01:56:15.679027 140411681085184 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "312/312 [==============================] - 116s 373ms/step - loss: 2.3670 - acc: 0.2759 - val_loss: 2.1861 - val_acc: 0.2046\n",
      "Epoch 2/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 2.3149 - acc: 0.2029 - val_loss: 2.0514 - val_acc: 0.2684\n",
      "Epoch 3/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.9420 - acc: 0.2923 - val_loss: 1.9431 - val_acc: 0.3323\n",
      "Epoch 4/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.7914 - acc: 0.3506 - val_loss: 1.6776 - val_acc: 0.3873\n",
      "Epoch 5/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.6811 - acc: 0.3882 - val_loss: 1.5927 - val_acc: 0.4136\n",
      "Epoch 6/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.5955 - acc: 0.4192 - val_loss: 1.5685 - val_acc: 0.4292\n",
      "Epoch 7/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.5354 - acc: 0.4427 - val_loss: 1.4944 - val_acc: 0.4549\n",
      "Epoch 8/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.4761 - acc: 0.4635 - val_loss: 1.5004 - val_acc: 0.4618\n",
      "Epoch 9/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.4343 - acc: 0.4781 - val_loss: 1.4110 - val_acc: 0.4917\n",
      "Epoch 10/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.3918 - acc: 0.4959 - val_loss: 1.4024 - val_acc: 0.4938\n",
      "Epoch 11/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.3480 - acc: 0.5123 - val_loss: 1.4295 - val_acc: 0.4860\n",
      "Epoch 12/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.3210 - acc: 0.5200 - val_loss: 1.2991 - val_acc: 0.5381\n",
      "Epoch 13/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.2923 - acc: 0.5342 - val_loss: 1.3405 - val_acc: 0.5201\n",
      "Epoch 14/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.2551 - acc: 0.5483 - val_loss: 1.2641 - val_acc: 0.5483\n",
      "Epoch 15/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.2315 - acc: 0.5555 - val_loss: 1.3095 - val_acc: 0.5290\n",
      "Epoch 16/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.1925 - acc: 0.5703 - val_loss: 1.3633 - val_acc: 0.5249\n",
      "Epoch 17/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.1644 - acc: 0.5804 - val_loss: 1.2033 - val_acc: 0.5723\n",
      "Epoch 18/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.1252 - acc: 0.5948 - val_loss: 1.1420 - val_acc: 0.6006\n",
      "Epoch 19/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.0972 - acc: 0.6050 - val_loss: 1.1741 - val_acc: 0.5820\n",
      "Epoch 20/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.0699 - acc: 0.6139 - val_loss: 1.2155 - val_acc: 0.5743\n",
      "Epoch 21/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.0459 - acc: 0.6221 - val_loss: 1.2175 - val_acc: 0.5748\n",
      "Epoch 22/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 1.0085 - acc: 0.6362 - val_loss: 1.1146 - val_acc: 0.6077\n",
      "Epoch 23/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 0.9922 - acc: 0.6446 - val_loss: 1.1178 - val_acc: 0.6099\n",
      "Epoch 24/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 0.9628 - acc: 0.6551 - val_loss: 1.1009 - val_acc: 0.6172\n",
      "Epoch 25/25\n",
      "312/312 [==============================] - 82s 264ms/step - loss: 0.9343 - acc: 0.6652 - val_loss: 1.0485 - val_acc: 0.6368\n",
      "Test loss    : 1.0591415632993748\n",
      "Test accuracy: 0.63000804\n",
      "CPU times: user 1h 16min 35s, sys: 13min 54s, total: 1h 30min 29s\n",
      "Wall time: 35min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyper-parameters\n",
    "    parser.add_argument('--epochs',        type=int,   default=25)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.01)\n",
    "    parser.add_argument('--batch-size',    type=int,   default=128)\n",
    "    parser.add_argument('--weight-decay',  type=float, default=2e-4)\n",
    "    parser.add_argument('--momentum',      type=float, default='0.9')\n",
    "    parser.add_argument('--optimizer',     type=str,   default='sgd')\n",
    "\n",
    "    # Data directories and other options\n",
    "    parser.add_argument('--gpu-count',        type=int,   default=4)\n",
    "    parser.add_argument('--model_output_dir', type=str,   default='./models')\n",
    "    parser.add_argument('--training',      type=str,   default='data/train/train.tfrecords')\n",
    "    parser.add_argument('--validation',    type=str,   default='data/validation/validation.tfrecords')\n",
    "    parser.add_argument('--eval',          type=str,   default='data/eval/eval.tfrecords')\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single gpu\n",
    "# CPU times: user 3min 47s, sys: 53.3 s, total: 4min 40s\n",
    "# Wall time: 2min 48s\n",
    "\n",
    "# 4 gpu\n",
    "# CPU times: user 16min 35s, sys: 2min 47s, total: 19min 22s\n",
    "# Wall time: 8min 12s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
