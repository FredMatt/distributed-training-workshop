{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from model_def import get_model\n",
    "\n",
    "HEIGHT = 32\n",
    "WIDTH  = 32\n",
    "DEPTH  = 3\n",
    "NUM_CLASSES = 10\n",
    "NUM_TRAIN_IMAGES = 40000\n",
    "NUM_VALID_IMAGES = 10000\n",
    "NUM_TEST_IMAGES  = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess_fn(image):\n",
    "\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, HEIGHT + 8, WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [HEIGHT, WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(filenames, batch_size):\n",
    "    \"\"\"Read the images and labels from 'filenames'.\"\"\"\n",
    "    # Repeat infinitely.\n",
    "    dataset = tf.data.TFRecordDataset(filenames).repeat()\n",
    "\n",
    "    # Parse records.\n",
    "    dataset = dataset.map(single_example_parser, num_parallel_calls=1)\n",
    "\n",
    "    # Batch it up.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    image_batch, label_batch = iterator.get_next()\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_example_parser(serialized_example):\n",
    "    \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "    # Dimensions of the images in the CIFAR-10 dataset.\n",
    "    # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n",
    "    # input format.\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image.set_shape([DEPTH * HEIGHT * WIDTH])\n",
    "\n",
    "    # Reshape from [depth * height * width] to [depth, height, width].\n",
    "    image = tf.cast(\n",
    "        tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),\n",
    "        tf.float32)\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    image = train_preprocess_fn(image)\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def main(args):\n",
    "    # Hyper-parameters\n",
    "    epochs = args.epochs\n",
    "    lr = args.learning_rate\n",
    "    batch_size = args.batch_size\n",
    "    momentum = args.momentum\n",
    "    weight_decay = args.weight_decay\n",
    "    optimizer = args.optimizer\n",
    "\n",
    "    # Data directories and other options\n",
    "    gpu_count = args.gpu_count\n",
    "    train_dir = args.train\n",
    "    validation_dir = args.validation\n",
    "    eval_dir = args.eval\n",
    "\n",
    "    train_dataset = make_batch(train_dir+'/train.tfrecords',  batch_size)\n",
    "    val_dataset = make_batch(validation_dir+'/validation.tfrecords', batch_size)\n",
    "    eval_dataset = make_batch(eval_dir+'/eval.tfrecords', batch_size)\n",
    "\n",
    "    input_shape = (HEIGHT, WIDTH, DEPTH)\n",
    "    model = get_model(lr, weight_decay, optimizer, momentum)\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(lr=lr, decay=weight_decay, momentum=momentum)\n",
    "    else:\n",
    "        opt = Adam(lr=lr, decay=weight_decay)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(x=train_dataset[0], y=train_dataset[1],\n",
    "                        steps_per_epoch=NUM_TRAIN_IMAGES // batch_size,\n",
    "                        validation_data=val_dataset,\n",
    "                        validation_steps=NUM_VALID_IMAGES // batch_size,\n",
    "                        epochs=epochs)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    score = model.evaluate(eval_dataset[0],\n",
    "                           eval_dataset[1],\n",
    "                           steps=NUM_TEST_IMAGES // args.batch_size,\n",
    "                           verbose=0)\n",
    "    print('Test loss    :', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # Save model to model directory\n",
    "    #tf.contrib.saved_model.save_keras_model(model, args.model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1018 07:51:09.111944 140524369356544 deprecation.py:323] From <ipython-input-3-281db9a6ad1c>:11: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W1018 07:51:09.249531 140524369356544 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 12s 37ms/step - loss: 1.9036 - acc: 0.2983 - val_loss: 2.3295 - val_acc: 0.2259\n",
      "Test loss    : 2.3136354226332445\n",
      "Test accuracy: 0.2265625\n",
      "CPU times: user 34.4 s, sys: 7.88 s, total: 42.2 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyper-parameters\n",
    "    parser.add_argument('--epochs',        type=int,   default=1)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.01)\n",
    "    parser.add_argument('--batch-size',    type=int,   default=128)\n",
    "    parser.add_argument('--weight-decay',  type=float, default=2e-4)\n",
    "    parser.add_argument('--momentum',      type=float, default='0.9')\n",
    "    parser.add_argument('--optimizer',     type=str,   default='sgd')\n",
    "\n",
    "    # Data directories and other options\n",
    "    parser.add_argument('--gpu-count',        type=int,   default=0)\n",
    "    parser.add_argument('--model_output_dir', type=str,   default='./models')\n",
    "    parser.add_argument('--train',      type=str,   default='../../data/train')\n",
    "    parser.add_argument('--validation',    type=str,   default='../../data/validation')\n",
    "    parser.add_argument('--eval',          type=str,   default='../../data/eval')\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
